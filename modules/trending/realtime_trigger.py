#!/usr/bin/env python3
"""
å®æ—¶æ–‡ç« ç”Ÿæˆè§¦å‘å™¨ - Realtime Content Generation Trigger
å½“æ£€æµ‹åˆ°é«˜ä»·å€¼çƒ­ç‚¹è¯é¢˜æ—¶ï¼Œç«‹å³è§¦å‘æ–‡ç« ç”Ÿæˆï¼Œä¸å—å®šæ—¶é™åˆ¶

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å®æ—¶ç›‘æ§çƒ­ç‚¹å…³é”®è¯å˜åŒ–
2. æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦ç«‹å³ç”Ÿæˆæ–‡ç« 
3. è‡ªåŠ¨è§¦å‘æ–‡ç« ç”Ÿæˆæµç¨‹
4. å³æ—¶Telegramé€šçŸ¥
5. é˜²æ­¢é‡å¤ç”Ÿæˆå’Œå†…å®¹å†²çª
"""

import os
import json
import asyncio
import subprocess
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any
from pathlib import Path
import pytz
import requests

# å¯¼å…¥å®æ—¶åˆ†æå™¨
from modules.trending.realtime_analyzer import RealtimeTrendingAnalyzer, TrendingTopic, analyze_current_trends


class RealtimeContentTrigger:
    """å®æ—¶å†…å®¹ç”Ÿæˆè§¦å‘å™¨"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.data_dir = "data/realtime_triggers"
        self.generation_history = "data/generation_history"
        self.monitoring_active = False
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.generation_history, exist_ok=True)
        
        # è§¦å‘æ¡ä»¶é…ç½®
        self.trigger_thresholds = {
            'min_trend_score': 0.75,        # æœ€ä½è¶‹åŠ¿è¯„åˆ†
            'min_commercial_value': 0.70,    # æœ€ä½å•†ä¸šä»·å€¼
            'min_urgency_score': 0.80,       # æœ€ä½ç´§æ€¥åº¦
            'min_search_volume': 10000,      # æœ€ä½æœç´¢é‡
            'max_competition_level': 'Medium-High'  # æœ€é«˜ç«äº‰åº¦
        }
        
        # é˜²é‡å¤ç”Ÿæˆé…ç½®
        self.cooldown_hours = 6  # åŒä¸€å…³é”®è¯6å°æ—¶å†…ä¸é‡å¤ç”Ÿæˆ
        self.max_daily_generations = 4  # æ¯æ—¥æœ€å¤§ç”Ÿæˆæ•°
        
        # Telegramé…ç½®
        self.telegram_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.telegram_chat_id = os.getenv('TELEGRAM_CHAT_ID')
        
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('data/realtime_trigger.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    async def start_monitoring(self, check_interval_minutes: int = 30):
        """å¯åŠ¨å®æ—¶ç›‘æ§"""
        self.monitoring_active = True
        self.logger.info(f"ğŸš€ å®æ—¶ç›‘æ§å¯åŠ¨ - æ£€æŸ¥é—´éš”: {check_interval_minutes} åˆ†é’Ÿ")
        
        while self.monitoring_active:
            try:
                await self.check_and_trigger()
                await asyncio.sleep(check_interval_minutes * 60)  # è½¬æ¢ä¸ºç§’
                
            except KeyboardInterrupt:
                self.logger.info("â¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­ç›‘æ§...")
                self.monitoring_active = False
                break
            except Exception as e:
                self.logger.error(f"âŒ ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­
    
    async def check_and_trigger(self) -> Dict[str, Any]:
        """æ£€æŸ¥çƒ­ç‚¹å¹¶è§¦å‘ç”Ÿæˆ"""
        self.logger.info("ğŸ” å¼€å§‹æ£€æŸ¥çƒ­ç‚¹è¯é¢˜...")
        
        # è·å–å½“å‰è¶‹åŠ¿åˆ†æ
        try:
            trends_data = await analyze_current_trends(force_analysis=False)
            trending_topics = [
                TrendingTopic(**topic) for topic in trends_data['trending_topics']
            ]
            
            if not trending_topics:
                self.logger.info("ğŸ“Š æœªå‘ç°æ–°çš„çƒ­ç‚¹è¯é¢˜")
                return {'status': 'no_trends', 'action': 'none'}
                
        except Exception as e:
            self.logger.error(f"âŒ è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'message': str(e)}
        
        # è¯„ä¼°æ˜¯å¦éœ€è¦ç«‹å³è§¦å‘
        trigger_candidates = self._evaluate_trigger_candidates(trending_topics)
        
        if not trigger_candidates:
            self.logger.info("â³ æš‚æ— ç¬¦åˆè§¦å‘æ¡ä»¶çš„çƒ­ç‚¹è¯é¢˜")
            return {'status': 'no_triggers', 'topics_count': len(trending_topics)}
        
        # æ‰§è¡Œè§¦å‘
        trigger_results = []
        for candidate in trigger_candidates:
            result = await self._execute_content_generation(candidate)
            trigger_results.append(result)
        
        # æ±‡æ€»æŠ¥å‘Š
        summary = self._generate_trigger_summary(trigger_results, trending_topics)
        
        # å‘é€Telegramé€šçŸ¥
        if trigger_results:
            await self._send_trigger_notification(summary)
        
        return summary
    
    def _evaluate_trigger_candidates(self, topics: List[TrendingTopic]) -> List[TrendingTopic]:
        """è¯„ä¼°è§¦å‘å€™é€‰è¯é¢˜"""
        candidates = []
        
        for topic in topics:
            # åŸºæœ¬æ¡ä»¶æ£€æŸ¥
            if not self._meets_basic_criteria(topic):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ€è¿‘å·²ç”Ÿæˆè¿‡ç›¸ä¼¼å†…å®¹
            if self._check_recent_generation(topic.keyword):
                self.logger.info(f"â­ï¸ è·³è¿‡ '{topic.keyword}' - æœ€è¿‘å·²ç”Ÿæˆç›¸ä¼¼å†…å®¹")
                continue
            
            # æ£€æŸ¥æ¯æ—¥ç”Ÿæˆé™é¢
            if self._check_daily_limit():
                self.logger.info("ğŸ“Š å·²è¾¾åˆ°æ¯æ—¥æœ€å¤§ç”Ÿæˆé™é¢")
                break
            
            # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼ŒåŠ å…¥å€™é€‰åˆ—è¡¨
            candidates.append(topic)
            self.logger.info(f"âœ… '{topic.keyword}' ç¬¦åˆç«‹å³ç”Ÿæˆæ¡ä»¶")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œå–å‰2ä¸ª
        candidates.sort(key=lambda t: (
            t.urgency_score * 0.4 + 
            t.commercial_value * 0.3 + 
            t.trend_score * 0.3
        ), reverse=True)
        
        return candidates[:2]  # æœ€å¤šåŒæ—¶ç”Ÿæˆ2ç¯‡æ–‡ç« 
    
    def _meets_basic_criteria(self, topic: TrendingTopic) -> bool:
        """æ£€æŸ¥è¯é¢˜æ˜¯å¦æ»¡è¶³åŸºæœ¬è§¦å‘æ¡ä»¶"""
        criteria_checks = {
            'trend_score': topic.trend_score >= self.trigger_thresholds['min_trend_score'],
            'commercial_value': topic.commercial_value >= self.trigger_thresholds['min_commercial_value'],
            'urgency_score': topic.urgency_score >= self.trigger_thresholds['min_urgency_score'],
            'search_volume': topic.search_volume_est >= self.trigger_thresholds['min_search_volume'],
            'competition_ok': self._check_competition_level(topic.competition_level)
        }
        
        passed_checks = sum(criteria_checks.values())
        required_checks = 4  # è‡³å°‘é€šè¿‡4é¡¹æ£€æŸ¥
        
        if passed_checks >= required_checks:
            self.logger.info(f"ğŸ¯ '{topic.keyword}' é€šè¿‡ {passed_checks}/5 é¡¹åŸºæœ¬æ£€æŸ¥")
            return True
        else:
            self.logger.debug(f"âŒ '{topic.keyword}' ä»…é€šè¿‡ {passed_checks}/5 é¡¹æ£€æŸ¥: {criteria_checks}")
            return False
    
    def _check_competition_level(self, competition: str) -> bool:
        """æ£€æŸ¥ç«äº‰åº¦æ˜¯å¦å¯æ¥å—"""
        competition_levels = {
            'Low': 1, 'Low-Medium': 2, 'Medium': 3, 
            'Medium-High': 4, 'High': 5
        }
        
        current_level = competition_levels.get(competition, 3)
        max_level = competition_levels.get(self.trigger_thresholds['max_competition_level'], 4)
        
        return current_level <= max_level
    
    def _check_recent_generation(self, keyword: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ€è¿‘å·²ç”Ÿæˆè¿‡ç›¸ä¼¼å†…å®¹"""
        history_file = f"{self.generation_history}/generation_log.json"
        
        if not os.path.exists(history_file):
            return False
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
            
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=self.cooldown_hours)
            
            for record in history[-20:]:  # æ£€æŸ¥æœ€è¿‘20æ¡è®°å½•
                if record['keyword'].lower() == keyword.lower():
                    gen_time = datetime.fromisoformat(record['timestamp'].replace('Z', '+00:00'))
                    if gen_time > cutoff_time:
                        return True
            
        except Exception as e:
            self.logger.error(f"âš ï¸ æ£€æŸ¥ç”Ÿæˆå†å²å¤±è´¥: {e}")
        
        return False
    
    def _check_daily_limit(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ¯æ—¥ç”Ÿæˆé™é¢"""
        history_file = f"{self.generation_history}/generation_log.json"
        
        if not os.path.exists(history_file):
            return False
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
            
            today = datetime.now(timezone.utc).date()
            today_count = 0
            
            for record in history[-50:]:  # æ£€æŸ¥æœ€è¿‘50æ¡è®°å½•
                gen_date = datetime.fromisoformat(record['timestamp'].replace('Z', '+00:00')).date()
                if gen_date == today:
                    today_count += 1
            
            return today_count >= self.max_daily_generations
            
        except Exception as e:
            self.logger.error(f"âš ï¸ æ£€æŸ¥æ¯æ—¥é™é¢å¤±è´¥: {e}")
            return False
    
    async def _execute_content_generation(self, topic: TrendingTopic) -> Dict[str, Any]:
        """æ‰§è¡Œå†…å®¹ç”Ÿæˆ"""
        self.logger.info(f"ğŸš€ å¼€å§‹ä¸º '{topic.keyword}' ç”Ÿæˆæ–‡ç« ...")
        
        start_time = datetime.now(timezone.utc)
        
        try:
            # è°ƒç”¨æ–‡ç« ç”Ÿæˆè„šæœ¬
            result = subprocess.run([
                'python', 'scripts/generate_daily_content.py',
                '--count', '1',
                '--keyword', topic.keyword,
                '--category', topic.category,
                '--priority', 'urgent'
            ], capture_output=True, text=True, timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            
            if result.returncode == 0:
                # ç”ŸæˆæˆåŠŸ
                generation_result = {
                    'status': 'success',
                    'keyword': topic.keyword,
                    'category': topic.category,
                    'start_time': start_time.isoformat(),
                    'end_time': datetime.now(timezone.utc).isoformat(),
                    'trigger_reason': topic.business_reasoning,
                    'estimated_revenue': topic.estimated_revenue,
                    'urgency_score': topic.urgency_score,
                    'stdout': result.stdout,
                    'stderr': result.stderr if result.stderr else None
                }
                
                # è®°å½•åˆ°å†å²
                self._log_generation(generation_result, topic)
                
                self.logger.info(f"âœ… '{topic.keyword}' æ–‡ç« ç”ŸæˆæˆåŠŸ")
                
            else:
                # ç”Ÿæˆå¤±è´¥
                generation_result = {
                    'status': 'failed',
                    'keyword': topic.keyword,
                    'error': result.stderr or 'Unknown error',
                    'return_code': result.returncode,
                    'start_time': start_time.isoformat(),
                    'end_time': datetime.now(timezone.utc).isoformat()
                }
                
                self.logger.error(f"âŒ '{topic.keyword}' æ–‡ç« ç”Ÿæˆå¤±è´¥: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            generation_result = {
                'status': 'timeout',
                'keyword': topic.keyword,
                'error': 'Generation process timed out after 5 minutes',
                'start_time': start_time.isoformat(),
                'end_time': datetime.now(timezone.utc).isoformat()
            }
            self.logger.error(f"â° '{topic.keyword}' æ–‡ç« ç”Ÿæˆè¶…æ—¶")
            
        except Exception as e:
            generation_result = {
                'status': 'error',
                'keyword': topic.keyword,
                'error': str(e),
                'start_time': start_time.isoformat(),
                'end_time': datetime.now(timezone.utc).isoformat()
            }
            self.logger.error(f"ğŸ’¥ '{topic.keyword}' ç”Ÿæˆè¿‡ç¨‹å¼‚å¸¸: {e}")
        
        return generation_result
    
    def _log_generation(self, result: Dict, topic: TrendingTopic):
        """è®°å½•ç”Ÿæˆå†å²"""
        history_file = f"{self.generation_history}/generation_log.json"
        
        log_entry = {
            'timestamp': result['end_time'],
            'keyword': topic.keyword,
            'category': topic.category,
            'status': result['status'],
            'trigger_type': 'realtime',
            'trend_score': topic.trend_score,
            'commercial_value': topic.commercial_value,
            'urgency_score': topic.urgency_score,
            'estimated_revenue': topic.estimated_revenue,
            'sources': topic.sources
        }
        
        # è¯»å–ç°æœ‰å†å²
        history = []
        if os.path.exists(history_file):
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except Exception as e:
                self.logger.error(f"è¯»å–å†å²è®°å½•å¤±è´¥: {e}")
        
        # æ·»åŠ æ–°è®°å½•
        history.append(log_entry)
        
        # ä¿æŒæœ€è¿‘100æ¡è®°å½•
        if len(history) > 100:
            history = history[-100:]
        
        # ä¿å­˜æ›´æ–°çš„å†å²
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
    
    def _generate_trigger_summary(self, results: List[Dict], all_topics: List[TrendingTopic]) -> Dict[str, Any]:
        """ç”Ÿæˆè§¦å‘æ±‡æ€»æŠ¥å‘Š"""
        successful_generations = [r for r in results if r['status'] == 'success']
        failed_generations = [r for r in results if r['status'] != 'success']
        
        summary = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'analysis_summary': {
                'total_topics_analyzed': len(all_topics),
                'triggers_attempted': len(results),
                'successful_generations': len(successful_generations),
                'failed_generations': len(failed_generations)
            },
            'generated_articles': successful_generations,
            'failed_attempts': failed_generations,
            'top_topics_not_triggered': [
                {
                    'keyword': topic.keyword,
                    'trend_score': topic.trend_score,
                    'reason_not_triggered': self._analyze_why_not_triggered(topic)
                }
                for topic in all_topics[:5] 
                if topic.keyword not in [r['keyword'] for r in results]
            ],
            'next_monitoring_cycle': (
                datetime.now(timezone.utc) + timedelta(minutes=30)
            ).isoformat()
        }
        
        return summary
    
    def _analyze_why_not_triggered(self, topic: TrendingTopic) -> str:
        """åˆ†æä¸ºä»€ä¹ˆè¯é¢˜æœªè¢«è§¦å‘"""
        reasons = []
        
        if topic.trend_score < self.trigger_thresholds['min_trend_score']:
            reasons.append(f"è¶‹åŠ¿è¯„åˆ†è¿‡ä½ ({topic.trend_score:.2f})")
        
        if topic.commercial_value < self.trigger_thresholds['min_commercial_value']:
            reasons.append(f"å•†ä¸šä»·å€¼ä¸è¶³ ({topic.commercial_value:.2f})")
        
        if topic.urgency_score < self.trigger_thresholds['min_urgency_score']:
            reasons.append(f"ç´§æ€¥åº¦ä¸å¤Ÿ ({topic.urgency_score:.2f})")
        
        if topic.search_volume_est < self.trigger_thresholds['min_search_volume']:
            reasons.append(f"æœç´¢é‡åä½ ({topic.search_volume_est:,})")
        
        if not self._check_competition_level(topic.competition_level):
            reasons.append(f"ç«äº‰è¿‡äºæ¿€çƒˆ ({topic.competition_level})")
        
        if self._check_recent_generation(topic.keyword):
            reasons.append("æœ€è¿‘å·²ç”Ÿæˆç›¸ä¼¼å†…å®¹")
        
        return "; ".join(reasons) if reasons else "æœªçŸ¥åŸå› "
    
    async def _send_trigger_notification(self, summary: Dict):
        """å‘é€Telegramè§¦å‘é€šçŸ¥"""
        if not self.telegram_token or not self.telegram_chat_id:
            self.logger.warning("âš ï¸ Telegramé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡é€šçŸ¥")
            return
        
        try:
            # æ„å»ºé€šçŸ¥æ¶ˆæ¯
            message = self._build_notification_message(summary)
            
            # å‘é€æ¶ˆæ¯
            url = f"https://api.telegram.org/bot{self.telegram_token}/sendMessage"
            payload = {
                'chat_id': self.telegram_chat_id,
                'text': message,
                'parse_mode': 'HTML',
                'disable_web_page_preview': True
            }
            
            response = requests.post(url, json=payload, timeout=10)
            
            if response.status_code == 200:
                self.logger.info("ğŸ“± Telegramè§¦å‘é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                self.logger.error(f"ğŸ“± Telegramé€šçŸ¥å‘é€å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"ğŸ“± å‘é€Telegramé€šçŸ¥å¼‚å¸¸: {e}")
    
    def _build_notification_message(self, summary: Dict) -> str:
        """æ„å»ºé€šçŸ¥æ¶ˆæ¯"""
        analysis = summary['analysis_summary']
        generated = summary['generated_articles']
        
        # åŸºç¡€ä¿¡æ¯
        message = f"""ğŸ”¥ <b>å®æ—¶çƒ­ç‚¹è§¦å‘æŠ¥å‘Š</b>
        
ğŸ“Š <b>åˆ†ææ¦‚å†µ</b>
â€¢ åˆ†æè¯é¢˜: {analysis['total_topics_analyzed']} ä¸ª
â€¢ è§¦å‘ç”Ÿæˆ: {analysis['triggers_attempted']} ä¸ª
â€¢ æˆåŠŸç”Ÿæˆ: {analysis['successful_generations']} ä¸ª
â€¢ å¤±è´¥ç”Ÿæˆ: {analysis['failed_generations']} ä¸ª

"""
        
        # æˆåŠŸç”Ÿæˆçš„æ–‡ç« 
        if generated:
            message += "âœ… <b>æˆåŠŸç”Ÿæˆæ–‡ç« </b>\n"
            for article in generated:
                message += f"â€¢ <code>{article['keyword']}</code>\n"
                message += f"  ğŸ’° é¢„ä¼°æ”¶ç›Š: {article.get('estimated_revenue', 'N/A')}\n"
                message += f"  ğŸ¯ ç´§æ€¥åº¦: {article.get('urgency_score', 0):.2f}\n\n"
        
        # æœªè§¦å‘çš„çƒ­é—¨è¯é¢˜
        not_triggered = summary.get('top_topics_not_triggered', [])
        if not_triggered:
            message += "â³ <b>ç›‘æ§ä¸­è¯é¢˜</b>\n"
            for topic in not_triggered[:3]:
                message += f"â€¢ <code>{topic['keyword']}</code> (è¯„åˆ†: {topic['trend_score']:.2f})\n"
        
        # ä¸‹æ¬¡æ£€æŸ¥æ—¶é—´
        next_check = datetime.fromisoformat(summary['next_monitoring_cycle'].replace('Z', '+00:00'))
        beijing_time = next_check.astimezone(pytz.timezone('Asia/Shanghai'))
        message += f"\nâ° ä¸‹æ¬¡æ£€æŸ¥: {beijing_time.strftime('%H:%M')}"
        
        return message
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        self.logger.info("â¹ï¸ å®æ—¶ç›‘æ§å·²åœæ­¢")


# æ‰‹åŠ¨è§¦å‘æ¥å£
async def manual_trigger_check(force: bool = True) -> Dict[str, Any]:
    """æ‰‹åŠ¨è§¦å‘æ£€æŸ¥ï¼ˆç”¨äºæµ‹è¯•æˆ–ç´§æ€¥æƒ…å†µï¼‰"""
    trigger = RealtimeContentTrigger()
    return await trigger.check_and_trigger()


# ä¸»è¦è¿è¡Œæ¥å£
async def start_realtime_monitoring(check_interval: int = 30):
    """å¯åŠ¨å®æ—¶ç›‘æ§ï¼ˆä¸»è¦æ¥å£ï¼‰"""
    trigger = RealtimeContentTrigger()
    await trigger.start_monitoring(check_interval)


# æµ‹è¯•å’Œæ¼”ç¤º
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å®æ—¶å†…å®¹ç”Ÿæˆè§¦å‘å™¨')
    parser.add_argument('--mode', choices=['monitor', 'check'], default='check',
                      help='è¿è¡Œæ¨¡å¼: monitor=æŒç»­ç›‘æ§, check=å•æ¬¡æ£€æŸ¥')
    parser.add_argument('--interval', type=int, default=30,
                      help='ç›‘æ§é—´éš”ï¼ˆåˆ†é’Ÿï¼‰')
    
    args = parser.parse_args()
    
    if args.mode == 'monitor':
        print(f"ğŸš€ å¯åŠ¨å®æ—¶ç›‘æ§æ¨¡å¼ - é—´éš”: {args.interval} åˆ†é’Ÿ")
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        asyncio.run(start_realtime_monitoring(args.interval))
    else:
        print("ğŸ” æ‰§è¡Œå•æ¬¡æ£€æŸ¥...")
        result = asyncio.run(manual_trigger_check())
        print(f"ğŸ“Š æ£€æŸ¥å®Œæˆ: {result['analysis_summary']['successful_generations']} ç¯‡æ–‡ç« å·²ç”Ÿæˆ")