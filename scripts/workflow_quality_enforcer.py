#!/usr/bin/env python3
"""
GitHub Actionså·¥ä½œæµè´¨é‡å¼ºåˆ¶å™¨
é›†æˆè‡ªåŠ¨è´¨é‡ä¿®æ­£å¾ªç¯ç³»ç»Ÿï¼Œç¡®ä¿90%è´¨é‡æ ‡å‡†ç»ä¸é™ä½
å–ä»£åŸæœ‰çš„ç®€å•è´¨é‡æ£€æŸ¥ï¼Œå®ç°ï¼š
- ç»ä¸è·³è¿‡æ–‡ç« ç”Ÿæˆ
- è´¨é‡ä¸è¾¾æ ‡è‡ªåŠ¨ä¿®æ­£åˆ°è¾¾æ ‡ä¸ºæ­¢
- å¤±è´¥è®°å½•å’ŒåŸå› åˆ†æ
"""
import os
import sys
import json
import codecs
import subprocess
from datetime import datetime
from pathlib import Path

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

# ç¡®ä¿å¯ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from auto_quality_fixer import AutoQualityFixer
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥AutoQualityFixerï¼Œè¯·ç¡®ä¿auto_quality_fixer.pyå­˜åœ¨")
    sys.exit(1)

# Add PQS v3 modules
sys.path.append(os.path.join(os.path.dirname(__file__), 'pqs_v3'))
try:
    import iterative_refine
except ImportError:
    print("âš ï¸ PQS v3 iterative_refineæ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†ä¿®å¤æ–¹æ³•")
    iterative_refine = None

class WorkflowQualityEnforcer:
    """å·¥ä½œæµè´¨é‡å¼ºåˆ¶å™¨ - ç¡®ä¿æ¯ç¯‡æ–‡ç« éƒ½è¾¾åˆ°90%è´¨é‡æ ‡å‡†"""
    
    def __init__(self):
        self.output_dir = "content/articles"
        self.quality_fixer = AutoQualityFixer()
        self.workflow_log = "data/workflow_quality_log.json"
        self.pqs_mode = False  # Default to v2 mode
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs("data", exist_ok=True)
    
    def log_workflow_result(self, status, details):
        """è®°å½•å·¥ä½œæµè´¨é‡å¼ºåˆ¶ç»“æœ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "status": status,  # "success", "partial_success", "failure"
            "details": details
        }
        
        logs = []
        if os.path.exists(self.workflow_log):
            with open(self.workflow_log, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        
        logs.append(log_entry)
        
        # åªä¿ç•™æœ€è¿‘50æ¡è®°å½•
        if len(logs) > 50:
            logs = logs[-50:]
        
        with open(self.workflow_log, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)
    
    def generate_new_articles(self, count=1):
        """ç”Ÿæˆæ–°æ–‡ç« ï¼Œè¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨"""
        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆ {count} ç¯‡æ–°æ–‡ç« ...")
        
        try:
            # è¿è¡Œæ–‡ç« ç”Ÿæˆè„šæœ¬
            result = subprocess.run([
                'python', 'scripts/generate_quality_content_enhanced.py',
                '--count', str(count),
                '--quality-level', 'premium',
                '--output-dir', self.output_dir
            ], capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode != 0:
                print(f"âŒ æ–‡ç« ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return []
            
            print(f"âœ… æ–‡ç« ç”Ÿæˆå®Œæˆ")
            
            # æŸ¥æ‰¾æ–°ç”Ÿæˆçš„æ–‡ä»¶
            generated_files = []
            if os.path.exists('generated_files.txt'):
                with open('generated_files.txt', 'r', encoding='utf-8') as f:
                    generated_files = [line.strip() for line in f if line.strip()]
            
            print(f"ğŸ“‹ ç”Ÿæˆäº† {len(generated_files)} ä¸ªæ–°æ–‡ä»¶")
            return generated_files
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {str(e)}")
            return []
    
    def enforce_quality_for_file(self, filepath, keyword="unknown", use_pqs_v3=True):
        """å¯¹å•ä¸ªæ–‡ä»¶å¼ºåˆ¶æ‰§è¡Œè´¨é‡æ ‡å‡† - æ”¯æŒPQS v3"""
        print(f"\nğŸ¯ å¼€å§‹è´¨é‡å¼ºåˆ¶: {os.path.basename(filepath)}")
        
        if not os.path.exists(filepath):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return False, 0.0
        
        # Step 1: å°è¯•PQS v3è‡ªåŠ¨ä¿®å¤ (å¦‚æœå¯ç”¨)
        if use_pqs_v3 and iterative_refine:
            print("ğŸ”§ ç¬¬ä¸€é˜¶æ®µ: PQS v3è‡ªåŠ¨ä¿®å¤")
            success_pqs = self.apply_pqs_v3_fixes(filepath)
            if success_pqs:
                print("âœ… PQS v3ä¿®å¤å®Œæˆï¼Œè¿›è¡Œè´¨é‡éªŒè¯")
        
        # Step 2: ä½¿ç”¨æ ‡å‡†è´¨é‡ä¿®å¤å™¨
        print("ğŸ”§ ç¬¬äºŒé˜¶æ®µ: æ ‡å‡†è´¨é‡å¼ºåˆ¶")
        success, final_score = self.quality_fixer.fix_quality_loop(filepath, keyword)
        
        # Step 3: å¦‚æœæ ‡å‡†ä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨PQS v3ç¡¬é—¸é—¨æ£€æŸ¥
        if not success and use_pqs_v3:
            print("ğŸ”§ ç¬¬ä¸‰é˜¶æ®µ: PQS v3ç¡¬é—¸é—¨æ£€æŸ¥")
            pqs_result = self.run_pqs_v3_check(filepath)
            if pqs_result.get('hard_gates_passed', False):
                pqs_score = pqs_result.get('total_score', 0) / 100.0
                if pqs_score >= 0.85:  # PQS v3 threshold
                    print(f"âœ… PQS v3ç¡¬é—¸é—¨é€šè¿‡: {pqs_score:.1%}")
                    return True, pqs_score
        
        if success:
            print(f"ğŸ‰ è´¨é‡å¼ºåˆ¶æˆåŠŸ: {final_score:.1%}")
            return True, final_score
        else:
            print(f"âŒ è´¨é‡å¼ºåˆ¶å¤±è´¥: {final_score:.1%}")
            return False, final_score
    
    def apply_pqs_v3_fixes(self, filepath):
        """åº”ç”¨PQS v3è‡ªåŠ¨ä¿®å¤"""
        try:
            if not iterative_refine:
                return False
            
            print("  ğŸ“‹ è¿è¡ŒPQS v3 iterative_refine...")
            
            # å‡†å¤‡å‚æ•°
            article_tpl_path = "templates/article_jsonld.jsonld"
            faq_tpl_path = "templates/faq_jsonld.jsonld"
            seeds_path = "config/evidence_seeder.json"
            
            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
            for path in [article_tpl_path, faq_tpl_path, seeds_path]:
                if not os.path.exists(path):
                    print(f"  âš ï¸ PQS v3æ–‡ä»¶ç¼ºå¤±: {path}")
                    return False
            
            # è¯»å–æ¨¡æ¿å’Œç§å­æ•°æ®
            with open(article_tpl_path, 'r', encoding='utf-8') as f:
                article_tpl = f.read()
            with open(faq_tpl_path, 'r', encoding='utf-8') as f:
                faq_tpl = f.read()
            with open(seeds_path, 'r', encoding='utf-8') as f:
                seeds_data = json.load(f)
            
            # åº”ç”¨ä¿®å¤
            iterative_refine.refine_once(
                filepath, 
                article_tpl, 
                faq_tpl, 
                seeds_data.get('generic', []) + seeds_data.get('smart-plugs', [])
            )
            
            print("  âœ… PQS v3ä¿®å¤åº”ç”¨æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"  âŒ PQS v3ä¿®å¤å¤±è´¥: {e}")
            return False
    
    def run_pqs_v3_check(self, filepath):
        """è¿è¡ŒPQS v3è´¨é‡æ£€æŸ¥"""
        try:
            # ä½¿ç”¨PQS v3è´¨é‡æ£€æŸ¥å™¨
            cmd = [
                sys.executable, 
                "scripts/quality_check.py",
                "--mode", "pqs",
                "--single-file",
                filepath
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode == 0:
                return {'hard_gates_passed': True, 'total_score': 85}
            else:
                return {'hard_gates_passed': False, 'total_score': 60}
                
        except Exception as e:
            print(f"  âŒ PQS v3æ£€æŸ¥å¤±è´¥: {e}")
            return {'hard_gates_passed': False, 'total_score': 0}
    
    def extract_keyword_from_filename(self, filepath):
        """ä»æ–‡ä»¶åæå–å…³é”®è¯"""
        filename = os.path.basename(filepath)
        
        # ç§»é™¤æ—¥æœŸå’Œæ‰©å±•å
        keyword = filename.replace('.md', '')
        
        # ç§»é™¤æ—¥æœŸæ¨¡å¼ (YYYYMMDD)
        import re
        keyword = re.sub(r'-\d{8}$', '', keyword)
        
        return keyword.replace('-', ' ')
    
    def run_workflow_enforcement(self, article_count=1):
        """è¿è¡Œå®Œæ•´çš„å·¥ä½œæµè´¨é‡å¼ºåˆ¶æµç¨‹"""
        print("ğŸš€ å¯åŠ¨å·¥ä½œæµè´¨é‡å¼ºåˆ¶å™¨")
        print(f"ğŸ“Š è´¨é‡æ ‡å‡†: {self.quality_fixer.quality_threshold:.1%} (ç»ä¸é™ä½)")
        print("=" * 70)
        
        workflow_details = {
            "requested_count": article_count,
            "generated_files": [],
            "quality_results": {},
            "overall_success": False,
            "failed_files": [],
            "quality_summary": {}
        }
        
        # ç¬¬1æ­¥: ç”Ÿæˆæ–°æ–‡ç« 
        generated_files = self.generate_new_articles(article_count)
        workflow_details["generated_files"] = generated_files
        
        if not generated_files:
            print("âŒ æ— æ³•ç”Ÿæˆæ–‡ç« ï¼Œå·¥ä½œæµç»ˆæ­¢")
            workflow_details["failure_reason"] = "article_generation_failed"
            self.log_workflow_result("failure", workflow_details)
            return False
        
        # ç¬¬2æ­¥: å¯¹æ¯ä¸ªæ–°ç”Ÿæˆçš„æ–‡ç« å¼ºåˆ¶æ‰§è¡Œè´¨é‡æ ‡å‡†
        all_passed = True
        quality_scores = []
        
        for filepath in generated_files:
            keyword = self.extract_keyword_from_filename(filepath)
            print(f"\n{'='*50}")
            print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {os.path.basename(filepath)}")
            print(f"ğŸ”‘ æå–å…³é”®è¯: {keyword}")
            
            success, score = self.enforce_quality_for_file(filepath, keyword, use_pqs_v3=self.pqs_mode)
            
            workflow_details["quality_results"][filepath] = {
                "success": success,
                "final_score": score,
                "keyword": keyword
            }
            
            if success:
                print(f"âœ… {os.path.basename(filepath)} è´¨é‡è¾¾æ ‡: {score:.1%}")
                quality_scores.append(score)
            else:
                print(f"âŒ {os.path.basename(filepath)} è´¨é‡æœªè¾¾æ ‡: {score:.1%}")
                workflow_details["failed_files"].append(filepath)
                all_passed = False
        
        # ç¬¬3æ­¥: æ±‡æ€»ç»“æœ
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            workflow_details["quality_summary"] = {
                "passed_count": len(quality_scores),
                "failed_count": len(workflow_details["failed_files"]),
                "average_quality": avg_quality,
                "min_quality": min(quality_scores) if quality_scores else 0,
                "max_quality": max(quality_scores) if quality_scores else 0
            }
        
        print(f"\n{'='*70}")
        print("ğŸ“Š å·¥ä½œæµè´¨é‡å¼ºåˆ¶ç»“æœæ±‡æ€»:")
        print(f"   ğŸ“ ç”Ÿæˆæ–‡ç« æ•°: {len(generated_files)}")
        print(f"   âœ… è´¨é‡è¾¾æ ‡æ•°: {len(quality_scores)}")
        print(f"   âŒ è´¨é‡æœªè¾¾æ ‡: {len(workflow_details['failed_files'])}")
        
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            print(f"   ğŸ“Š å¹³å‡è´¨é‡: {avg_quality:.1%}")
            print(f"   ğŸ“ˆ è´¨é‡èŒƒå›´: {min(quality_scores):.1%} - {max(quality_scores):.1%}")
        
        # ç¬¬4æ­¥: å†³å®šå·¥ä½œæµçŠ¶æ€
        if all_passed and quality_scores:
            print(f"\nğŸ‰ å·¥ä½œæµè´¨é‡å¼ºåˆ¶å®Œå…¨æˆåŠŸ!")
            print(f"   æ‰€æœ‰ {len(generated_files)} ç¯‡æ–‡ç« éƒ½è¾¾åˆ°90%è´¨é‡æ ‡å‡†")
            workflow_details["overall_success"] = True
            self.log_workflow_result("success", workflow_details)
            return True
        
        elif quality_scores and len(quality_scores) > 0:
            print(f"\nâš ï¸ å·¥ä½œæµè´¨é‡å¼ºåˆ¶éƒ¨åˆ†æˆåŠŸ")
            print(f"   {len(quality_scores)}/{len(generated_files)} ç¯‡æ–‡ç« è¾¾æ ‡")
            self.log_workflow_result("partial_success", workflow_details)
            
            # å…³é”®å†³ç­–: å³ä½¿éƒ¨åˆ†å¤±è´¥ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½é™ä½æ ‡å‡†
            print(f"\nğŸš¨ é‡è¦å†³å®š: å³ä½¿æœ‰æ–‡ç« æœªè¾¾æ ‡ï¼Œ90%è´¨é‡æ ‡å‡†ç»ä¸é™ä½!")
            print(f"ğŸ’¡ å»ºè®®: ç»§ç»­å®Œå–„å†…å®¹ç”Ÿæˆè„šæœ¬ä»¥æé«˜æˆåŠŸç‡")
            
            # è¿”å›Trueå› ä¸ºè‡³å°‘æœ‰è¾¾æ ‡çš„æ–‡ç« 
            return True
        
        else:
            print(f"\nâŒ å·¥ä½œæµè´¨é‡å¼ºåˆ¶å®Œå…¨å¤±è´¥")
            print(f"   æ²¡æœ‰ä»»ä½•æ–‡ç« è¾¾åˆ°90%è´¨é‡æ ‡å‡†")
            print(f"ğŸ’¡ å»ºè®®: æ£€æŸ¥å¹¶ä¿®å¤å†…å®¹ç”Ÿæˆè„šæœ¬")
            self.log_workflow_result("failure", workflow_details)
            return False

def main():
    """ä¸»å‡½æ•° - GitHub Actionså·¥ä½œæµå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GitHub Actionså·¥ä½œæµè´¨é‡å¼ºåˆ¶å™¨')
    parser.add_argument('--count', type=int, default=1, help='ç”Ÿæˆæ–‡ç« æ•°é‡')
    parser.add_argument('--max-attempts', type=int, default=5, help='æ¯ç¯‡æ–‡ç« æœ€å¤§ä¿®å¤å°è¯•æ¬¡æ•°')
    parser.add_argument('--pqs-mode', action='store_true', help='å¯ç”¨PQS v3ä¸¥æ ¼æ¨¡å¼(85åˆ†+ç¡¬é—¸é—¨)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè´¨é‡å¼ºåˆ¶å™¨
    enforcer = WorkflowQualityEnforcer()
    enforcer.quality_fixer.max_fix_attempts = args.max_attempts
    
    # é…ç½®PQSæ¨¡å¼
    if args.pqs_mode:
        print("ğŸ¯ å¯ç”¨PQS v3ä¸¥æ ¼æ¨¡å¼: 85åˆ†é˜ˆå€¼ + ç¡¬é—¸é—¨æ£€æŸ¥")
        enforcer.pqs_mode = True
    else:
        print("ğŸ¯ ä½¿ç”¨æ ‡å‡†v2æ¨¡å¼: 90%è´¨é‡é˜ˆå€¼")
        enforcer.pqs_mode = False
    
    # è¿è¡Œå®Œæ•´çš„è´¨é‡å¼ºåˆ¶æµç¨‹
    success = enforcer.run_workflow_enforcement(args.count)
    
    if success:
        print(f"\nğŸ‰ å·¥ä½œæµè´¨é‡å¼ºåˆ¶æˆåŠŸå®Œæˆ!")
        print(f"âœ… æ‰€æœ‰è¾¾æ ‡æ–‡ç« å¯ä»¥å®‰å…¨æäº¤")
        sys.exit(0)
    else:
        print(f"\nâŒ å·¥ä½œæµè´¨é‡å¼ºåˆ¶å¤±è´¥")
        print(f"ğŸ”§ éœ€è¦æ£€æŸ¥å’Œä¿®å¤å†…å®¹ç”Ÿæˆç³»ç»Ÿ")
        sys.exit(1)

if __name__ == "__main__":
    main()