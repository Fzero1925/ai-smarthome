#!/usr/bin/env python3
"""
æ™ºèƒ½å›¾ç‰‡å»é‡å’Œå¢å¼ºè„šæœ¬ v2 Enhanced
è§£å†³æ–‡ç« é—´å›¾ç‰‡é‡å¤ä½¿ç”¨é—®é¢˜ï¼Œä¸ºç¼ºå°‘å›¾ç‰‡çš„æ–‡ç« æ·»åŠ åˆé€‚å›¾ç‰‡
æ–°å¢ï¼šå†…å®¹å“ˆå¸Œå»é‡ã€Smart Image Manageré›†æˆã€é…ç½®åŒ–ç®¡ç†
"""

import os
import sys
import codecs
import re
import json
import hashlib
from pathlib import Path
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

# æ·»åŠ modulesè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.image_tools.product_image_mapper import ProductImageMapper

# å°è¯•å¯¼å…¥Smart Image Manager
try:
    from smart_image_manager import search_and_assign
    SMART_IMAGES_AVAILABLE = True
    print("âœ… Smart Image Manager integration enabled")
except ImportError:
    SMART_IMAGES_AVAILABLE = False
    print("âš ï¸ Smart Image Manager not available, using fallback methods")

# å°è¯•å¯¼å…¥ç»¼åˆImageManager
try:
    from modules.image_tools.image_manager import ComprehensiveImageManager
    COMPREHENSIVE_MANAGER_AVAILABLE = True
except ImportError:
    COMPREHENSIVE_MANAGER_AVAILABLE = False

class ImageDeduplicator:
    """å›¾ç‰‡å»é‡å’Œå¢å¼ºå™¨ v2 Enhanced"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.mapper = ProductImageMapper()
        self.content_dir = Path("content/articles")
        self.image_dir = Path("static/images")
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # åˆå§‹åŒ–Smart Image Manager (å¦‚æœå¯ç”¨)
        self.smart_manager = None
        if COMPREHENSIVE_MANAGER_AVAILABLE:
            try:
                self.smart_manager = ComprehensiveImageManager(config_path)
                print("âœ… Comprehensive Image Manager initialized")
            except Exception as e:
                print(f"âš ï¸ Comprehensive Image Manager init failed: {e}")
        
        # å†…å®¹å“ˆå¸Œç¼“å­˜
        self.hash_cache = {}
        self.hash_cache_file = Path("data/image_hash_cache.json")
        self._load_hash_cache()
        
        print(f"ğŸ“ Content directory: {self.content_dir}")
        print(f"ğŸ–¼ï¸ Image directory: {self.image_dir}")
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not config_path:
            config_path = project_root / 'image_config.yml'
        
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                print(f"âœ… Loaded config from {config_path}")
                return config
        except Exception as e:
            print(f"âš ï¸ Failed to load config, using defaults: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'quality': {
                'max_usage_count': 3,
                'min_image_relevance_score': 0.6
            },
            'file_organization': {
                'use_content_hash': True,
                'hash_length': 8
            },
            'cache': {
                'cache_directory': 'data/image_cache'
            }
        }
    
    def _load_hash_cache(self):
        """åŠ è½½å“ˆå¸Œç¼“å­˜"""
        try:
            if self.hash_cache_file.exists():
                with open(self.hash_cache_file, 'r', encoding='utf-8') as f:
                    self.hash_cache = json.load(f)
                print(f"ğŸ“ Loaded {len(self.hash_cache)} cached hashes")
        except Exception as e:
            print(f"âš ï¸ Failed to load hash cache: {e}")
            self.hash_cache = {}
    
    def _save_hash_cache(self):
        """ä¿å­˜å“ˆå¸Œç¼“å­˜"""
        try:
            self.hash_cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.hash_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.hash_cache, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ Failed to save hash cache: {e}")
    
    def _content_hash(self, image_path: Path) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶å†…å®¹å“ˆå¸Œ"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{image_path}:{image_path.stat().st_mtime}"
            if cache_key in self.hash_cache:
                return self.hash_cache[cache_key]
            
            # è®¡ç®—å“ˆå¸Œ
            hash_obj = hashlib.sha1(image_path.read_bytes())
            content_hash = hash_obj.hexdigest()
            
            # ç¼“å­˜ç»“æœ
            self.hash_cache[cache_key] = content_hash
            
            return content_hash
            
        except Exception as e:
            print(f"âš ï¸ Failed to compute hash for {image_path}: {e}")
            return None
        
    def analyze_image_usage_across_articles(self):
        """åˆ†ææ‰€æœ‰æ–‡ç« çš„å›¾ç‰‡ä½¿ç”¨æƒ…å†µ"""
        image_usage = defaultdict(list)  # image_path -> [article_files]
        article_images = {}  # article_file -> [image_paths]
        
        for article_file in self.content_dir.glob("*.md"):
            try:
                with open(article_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–æ‰€æœ‰å›¾ç‰‡
                image_pattern = r'!\[([^\]]*)\]\(([^)]+?)(?:\s+"([^"]*)")?\)'
                matches = re.findall(image_pattern, content)
                
                article_images[article_file.name] = []
                for match in matches:
                    alt_text, image_path, title = match
                    if image_path.startswith('/images/products/'):
                        image_usage[image_path].append(article_file.name)
                        article_images[article_file.name].append(image_path)
                        
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ç« å¤±è´¥ {article_file}: {e}")
        
        return dict(image_usage), article_images
    
    def find_duplicate_images(self, image_usage):
        """æ‰¾å‡ºè¢«å¤šç¯‡æ–‡ç« ä½¿ç”¨çš„å›¾ç‰‡"""
        return {img: articles for img, articles in image_usage.items() if len(articles) > 1}
    
    def extract_article_info(self, article_file):
        """æå–æ–‡ç« åŸºæœ¬ä¿¡æ¯"""
        try:
            with open(self.content_dir / article_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ ‡é¢˜
            title_match = re.search(r'title:\s*["\'](.+?)["\']', content)
            title = title_match.group(1) if title_match else ""
            
            # æå–å…³é”®è¯
            keywords_match = re.search(r'keywords:\s*\[(.+?)\]', content)
            if keywords_match:
                keywords_str = keywords_match.group(1)
                keywords = [kw.strip(' "\'') for kw in keywords_str.split(',')]
                primary_keyword = keywords[0] if keywords else ""
            else:
                # ä»æ–‡ä»¶åæå–
                base_name = article_file.replace('.md', '')
                base_name = re.sub(r'-\d{8}$', '', base_name)
                primary_keyword = base_name.replace('-', ' ')
            
            # æå–æ—¥æœŸ
            date_match = re.search(r'date:\s*(.+?)Z', content)
            date_str = date_match.group(1) if date_match else "1900-01-01T00:00:00"
            
            return {
                'title': title,
                'keyword': primary_keyword,
                'content': content,
                'date': date_str,
                'file': article_file
            }
            
        except Exception as e:
            print(f"âŒ æå–æ–‡ç« ä¿¡æ¯å¤±è´¥ {article_file}: {e}")
            return None
    
    def resolve_image_conflicts(self, duplicate_images, article_images):
        """è§£å†³å›¾ç‰‡å†²çªï¼Œä¸ºæ¯ç¯‡æ–‡ç« åˆ†é…æœ€åˆé€‚çš„å›¾ç‰‡"""
        resolutions = {}
        
        for image_path, conflicting_articles in duplicate_images.items():
            print(f"\nğŸ” è§£å†³å›¾ç‰‡å†²çª: {image_path}")
            print(f"   å†²çªæ–‡ç« : {', '.join(conflicting_articles)}")
            
            # åˆ†ææ¯ç¯‡æ–‡ç« å¯¹è¿™ä¸ªå›¾ç‰‡çš„é€‚é…åº¦
            article_scores = []
            
            for article_file in conflicting_articles:
                article_info = self.extract_article_info(article_file)
                if not article_info:
                    continue
                
                # è®¡ç®—å›¾ç‰‡åŒ¹é…åˆ†æ•°
                matches = self.mapper.analyze_keyword_match(
                    article_info['keyword'], 
                    article_info['content']
                )
                
                # æ‰¾åˆ°å½“å‰å›¾ç‰‡çš„åˆ†æ•°
                image_score = 0
                for img_path, score, metadata in matches:
                    if img_path == image_path:
                        image_score = score
                        break
                
                article_scores.append({
                    'article': article_file,
                    'score': image_score,
                    'keyword': article_info['keyword'],
                    'date': article_info['date']
                })
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œåˆ†æ•°ç›¸åŒåˆ™æŒ‰æ—¥æœŸï¼ˆè¾ƒæ–°çš„ä¼˜å…ˆï¼‰
            article_scores.sort(key=lambda x: (x['score'], x['date']), reverse=True)
            
            if article_scores:
                # æœ€åŒ¹é…çš„æ–‡ç« ä¿ç•™åŸå›¾ç‰‡
                winner = article_scores[0]
                print(f"   âœ… æœ€ä½³åŒ¹é…: {winner['article']} (åˆ†æ•°: {winner['score']:.2f})")
                
                # å…¶ä»–æ–‡ç« éœ€è¦æ›¿æ¢å›¾ç‰‡
                for loser in article_scores[1:]:
                    article_file = loser['article']
                    article_info = self.extract_article_info(article_file)
                    
                    # ä¸ºè¿™ç¯‡æ–‡ç« æ‰¾åˆ°æ›¿ä»£å›¾ç‰‡ï¼ˆæ’é™¤å½“å‰å†²çªå›¾ç‰‡ï¼‰
                    exclude_images = [image_path] + article_images.get(article_file, [])
                    
                    best_alternative = self.mapper.get_best_image_for_keyword(
                        article_info['keyword'],
                        article_info['content'],
                        exclude_images
                    )
                    
                    if best_alternative:
                        alt_image_path, alt_metadata = best_alternative
                        resolutions[f"{article_file}:{image_path}"] = {
                            'article': article_file,
                            'old_image': image_path,
                            'new_image': alt_image_path,
                            'new_alt': alt_metadata['alt_text'],
                            'reason': f'è§£å†³å†²çªï¼Œæ›¿æ¢ä¸ºæ›´åˆé€‚çš„å›¾ç‰‡ (åˆ†æ•°: {alt_metadata.get("score", 0):.2f})'
                        }
                        print(f"   ğŸ”„ {article_file}: æ›¿æ¢ä¸º {alt_image_path}")
                    else:
                        print(f"   âš ï¸  {article_file}: æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ›¿ä»£å›¾ç‰‡")
        
        return resolutions
    
    def add_missing_images(self, article_images):
        """ä¸ºç¼ºå°‘å›¾ç‰‡çš„æ–‡ç« æ·»åŠ å›¾ç‰‡"""
        additions = {}
        
        for article_file in self.content_dir.glob("*.md"):
            if article_file.name not in article_images or not article_images[article_file.name]:
                # è¿™ç¯‡æ–‡ç« ç¼ºå°‘å›¾ç‰‡
                article_info = self.extract_article_info(article_file.name)
                if not article_info:
                    continue
                
                print(f"\nğŸ“· ä¸ºæ–‡ç« æ·»åŠ å›¾ç‰‡: {article_file.name}")
                print(f"   å…³é”®è¯: {article_info['keyword']}")
                
                # æ‰¾åˆ°æœ€ä½³å›¾ç‰‡
                best_match = self.mapper.get_best_image_for_keyword(
                    article_info['keyword'],
                    article_info['content']
                )
                
                if best_match:
                    image_path, metadata = best_match
                    additions[article_file.name] = {
                        'article': article_file.name,
                        'image': image_path,
                        'alt_text': metadata['alt_text'],
                        'keyword': article_info['keyword'],
                        'insert_position': 'after_title'  # åœ¨æ ‡é¢˜åæ’å…¥
                    }
                    print(f"   âœ… å»ºè®®æ·»åŠ : {image_path}")
                else:
                    print(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å›¾ç‰‡")
        
        return additions
    
    def apply_image_changes(self, resolutions, additions, auto_apply=False):
        """åº”ç”¨å›¾ç‰‡æ›´æ”¹"""
        if not resolutions and not additions:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®æ”¹çš„å›¾ç‰‡")
            return
        
        print(f"\nğŸ“ å˜æ›´æ‘˜è¦:")
        print(f"   - å›¾ç‰‡æ›¿æ¢: {len(resolutions)} é¡¹")
        print(f"   - å›¾ç‰‡æ·»åŠ : {len(additions)} é¡¹")
        
        if not auto_apply:
            print(f"\nğŸ’¡ ä½¿ç”¨ --apply å‚æ•°è‡ªåŠ¨åº”ç”¨è¿™äº›æ›´æ”¹")
            return
        
        # åº”ç”¨å›¾ç‰‡æ›¿æ¢
        for change_id, resolution in resolutions.items():
            self.apply_image_replacement(resolution)
        
        # åº”ç”¨å›¾ç‰‡æ·»åŠ 
        for article_file, addition in additions.items():
            self.apply_image_addition(addition)
        
        print(f"\nâœ… æ‰€æœ‰å›¾ç‰‡æ›´æ”¹å·²åº”ç”¨å®Œæˆï¼")
    
    def apply_image_replacement(self, resolution):
        """åº”ç”¨å•ä¸ªå›¾ç‰‡æ›¿æ¢"""
        article_file = self.content_dir / resolution['article']
        
        try:
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ‰¾åˆ°å¹¶æ›¿æ¢å›¾ç‰‡å¼•ç”¨
            old_pattern = rf'!\[([^\]]*)\]\({re.escape(resolution["old_image"])}(?:\s+"([^"]*)")?\)'
            
            def replacement(match):
                old_alt = match.group(1)
                old_title = match.group(2) if match.group(2) else ""
                
                # ä½¿ç”¨æ–°çš„altæ–‡æœ¬ï¼Œä¿ç•™åŸæ ‡é¢˜æˆ–ä½¿ç”¨æ–°çš„
                new_title = old_title if old_title else resolution['new_alt']
                return f'![{resolution["new_alt"]}]({resolution["new_image"]} "{new_title}")'
            
            new_content = re.sub(old_pattern, replacement, content)
            
            # å†™å…¥æ–‡ä»¶
            with open(article_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"âœ… å·²æ›¿æ¢å›¾ç‰‡: {resolution['article']}")
            print(f"   {resolution['old_image']} â†’ {resolution['new_image']}")
            
        except Exception as e:
            print(f"âŒ æ›¿æ¢å›¾ç‰‡å¤±è´¥ {resolution['article']}: {e}")
    
    def apply_image_addition(self, addition):
        """ä¸ºæ–‡ç« æ·»åŠ å›¾ç‰‡"""
        article_file = self.content_dir / addition['article']
        
        try:
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åœ¨introductionæ ‡é¢˜åæ’å…¥å›¾ç‰‡
            intro_pattern = r'(## Introduction\s*\n)'
            
            image_markdown = f'\n![{addition["alt_text"]}]({addition["image"]} "{addition["alt_text"]}")\n\n*Featured: Professional review and buying guide for {addition["keyword"]}*\n'
            
            if re.search(intro_pattern, content):
                new_content = re.sub(intro_pattern, r'\1' + image_markdown, content)
            else:
                # å¦‚æœæ²¡æœ‰Introductionæ ‡é¢˜ï¼Œåœ¨front matteråæ’å…¥
                front_matter_end = content.find('---', 3) + 3
                if front_matter_end > 2:
                    new_content = content[:front_matter_end] + '\n' + image_markdown + content[front_matter_end:]
                else:
                    new_content = image_markdown + content
            
            # å†™å…¥æ–‡ä»¶
            with open(article_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"âœ… å·²æ·»åŠ å›¾ç‰‡: {addition['article']}")
            print(f"   å›¾ç‰‡: {addition['image']}")
            
        except Exception as e:
            print(f"âŒ æ·»åŠ å›¾ç‰‡å¤±è´¥ {addition['article']}: {e}")
    
    def run_full_deduplication(self, auto_apply=False):
        """è¿è¡Œå®Œæ•´çš„å›¾ç‰‡å»é‡å’Œå¢å¼ºæµç¨‹"""
        print("ğŸš€ å¼€å§‹å›¾ç‰‡å»é‡å’Œå¢å¼ºæµç¨‹")
        print("=" * 60)
        
        # 1. åˆ†æå›¾ç‰‡ä½¿ç”¨æƒ…å†µ
        print("ğŸ“Š åˆ†æå›¾ç‰‡ä½¿ç”¨æƒ…å†µ...")
        image_usage, article_images = self.analyze_image_usage_across_articles()
        
        print(f"   - å‘ç° {len(image_usage)} ä¸ªå›¾ç‰‡è¢«ä½¿ç”¨")
        print(f"   - æ¶‰åŠ {len(article_images)} ç¯‡æ–‡ç« ")
        
        # 2. æ‰¾å‡ºé‡å¤å›¾ç‰‡
        duplicate_images = self.find_duplicate_images(image_usage)
        print(f"   - å‘ç° {len(duplicate_images)} ä¸ªé‡å¤ä½¿ç”¨çš„å›¾ç‰‡")
        
        # 3. è§£å†³å›¾ç‰‡å†²çª
        resolutions = {}
        if duplicate_images:
            print(f"\nğŸ”§ è§£å†³å›¾ç‰‡å†²çª...")
            resolutions = self.resolve_image_conflicts(duplicate_images, article_images)
        
        # 4. ä¸ºç¼ºå°‘å›¾ç‰‡çš„æ–‡ç« æ·»åŠ å›¾ç‰‡
        print(f"\nğŸ“· æ£€æŸ¥ç¼ºå°‘å›¾ç‰‡çš„æ–‡ç« ...")
        additions = self.add_missing_images(article_images)
        
        # 5. åº”ç”¨æ‰€æœ‰æ›´æ”¹
        self.apply_image_changes(resolutions, additions, auto_apply)
        
        return len(resolutions) + len(additions)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Smart Image Deduplication and Enhancement v2')
    parser.add_argument('--apply', '-a', action='store_true', help='Apply changes automatically')
    parser.add_argument('--mode', choices=['usage', 'content', 'both'], default='usage', 
                        help='Deduplication mode: usage (article usage), content (hash-based), or both')
    parser.add_argument('--execute', action='store_true', help='Execute content hash deduplication plan')
    parser.add_argument('--dry-run', action='store_true', default=True, help='Show changes without applying (default)')
    parser.add_argument('--image-root', default='static/images', help='Root directory for images')
    parser.add_argument('--config', help='Path to configuration file')
    
    args = parser.parse_args()
    
    # Override dry-run if apply is specified
    if args.apply:
        args.dry_run = False
    
    total_changes = 0
    
    # Run usage-based deduplication
    if args.mode in ['usage', 'both']:
        print("ğŸ¯ Running Usage-Based Deduplication...")
        print("=" * 60)
        
        deduplicator = ImageDeduplicator(config_path=args.config)
        
        if args.apply:
            print("âš ï¸  è‡ªåŠ¨åº”ç”¨æ¨¡å¼ - å°†ç›´æ¥ä¿®æ”¹æ–‡ç« æ–‡ä»¶")
            response = input("ç¡®è®¤ç»§ç»­? (y/N): ")
            if response.lower() != 'y':
                print("æ“ä½œå·²å–æ¶ˆ")
                sys.exit(0)
        
        changes_count = deduplicator.run_full_deduplication(auto_apply=args.apply)
        total_changes += changes_count
        
        if changes_count > 0 and not args.apply:
            print(f"\nğŸ’¡ å‘ç° {changes_count} å¤„å¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹")
            print("ä½¿ç”¨ --apply å‚æ•°è‡ªåŠ¨åº”ç”¨ä¿®å¤")
        elif changes_count == 0:
            print(f"\nâœ… ä½¿ç”¨æƒ…å†µå»é‡å®Œç¾ï¼Œæ— éœ€ä¿®æ”¹ï¼")
        
        # ä¿å­˜å“ˆå¸Œç¼“å­˜
        deduplicator._save_hash_cache()
    
    # Run content hash-based deduplication
    if args.mode in ['content', 'both']:
        if args.mode == 'both':
            print(f"\n" + "=" * 60)
        
        print("ğŸ” Running Content Hash-Based Deduplication...")
        print("=" * 60)
        
        hash_changes = run_content_hash_deduplication(
            image_root=args.image_root,
            dry_run=args.dry_run,
            execute=args.execute or args.apply
        )
        total_changes += hash_changes
    
    # Summary
    if args.mode == 'both':
        print(f"\n" + "=" * 60)
        print(f"ğŸ‰ Complete Deduplication Analysis Finished!")
        print(f"ğŸ“Š Total optimization opportunities: {total_changes}")
        
        if total_changes > 0 and args.dry_run:
            print(f"\nğŸ’¡ To apply all changes:")
            print(f"   - Usage-based: python {sys.argv[0]} --mode usage --apply")
            print(f"   - Content-based: python {sys.argv[0]} --mode content --execute --apply")
            print(f"   - Both: python {sys.argv[0]} --mode both --apply --execute")
    else:
        print(f"\nğŸ‰ {args.mode.title()}-based deduplication analysis complete!")


# === v2: Enhanced Content Hash-Based Deduplication System ===

class ContentHashDeduplicator:
    """v2 å†…å®¹å“ˆå¸Œå»é‡ç³»ç»Ÿ - æ£€æµ‹å®é™…ç›¸åŒçš„å›¾ç‰‡æ–‡ä»¶"""
    
    def __init__(self, image_root: str = 'static/images'):
        self.image_root = Path(image_root)
        self.hash_cache = {}
        self.supported_formats = {'.webp', '.jpg', '.jpeg', '.png', '.gif'}
        
    def _content_hash(self, file_path: Path) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶å†…å®¹å“ˆå¸Œ"""
        try:
            return hashlib.sha1(file_path.read_bytes()).hexdigest()
        except Exception as e:
            print(f"âš ï¸ Failed to hash {file_path}: {e}")
            return None
    
    def find_content_duplicates(self, verbose: bool = True) -> List[Tuple[Path, Path]]:
        """æŸ¥æ‰¾å†…å®¹å®Œå…¨ç›¸åŒçš„å›¾ç‰‡æ–‡ä»¶"""
        print(f"ğŸ” Scanning {self.image_root} for content duplicates...")
        
        seen_hashes = {}
        duplicates = []
        
        # æ‰«ææ‰€æœ‰æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        for ext in self.supported_formats:
            for file_path in self.image_root.rglob(f'*{ext}'):
                if not file_path.is_file():
                    continue
                
                content_hash = self._content_hash(file_path)
                if not content_hash:
                    continue
                
                if content_hash in seen_hashes:
                    duplicates.append((file_path, seen_hashes[content_hash]))
                    if verbose:
                        print(f"   ğŸ”„ Duplicate found: {file_path} == {seen_hashes[content_hash]}")
                else:
                    seen_hashes[content_hash] = file_path
        
        print(f"âœ… Content duplicate scan complete: found {len(duplicates)} duplicate pairs")
        return duplicates
    
    def analyze_duplicate_usage(self, duplicates: List[Tuple[Path, Path]], content_dir: Path = Path("content/articles")) -> Dict:
        """åˆ†æé‡å¤å›¾ç‰‡çš„ä½¿ç”¨æƒ…å†µ"""
        usage_analysis = []
        
        for dup_path, original_path in duplicates:
            # è½¬æ¢ä¸ºç½‘ç«™ç›¸å¯¹è·¯å¾„
            try:
                dup_web_path = '/' + str(dup_path.relative_to(Path('static')))
                original_web_path = '/' + str(original_path.relative_to(Path('static')))
            except ValueError:
                continue
            
            # æŸ¥æ‰¾å¼•ç”¨è¿™äº›å›¾ç‰‡çš„æ–‡ç« 
            dup_articles = self._find_articles_using_image(dup_web_path, content_dir)
            original_articles = self._find_articles_using_image(original_web_path, content_dir)
            
            if dup_articles or original_articles:
                usage_analysis.append({
                    'duplicate_file': str(dup_path),
                    'original_file': str(original_path),
                    'duplicate_web_path': dup_web_path,
                    'original_web_path': original_web_path,
                    'duplicate_used_in': dup_articles,
                    'original_used_in': original_articles,
                    'total_usage': len(dup_articles) + len(original_articles),
                    'can_consolidate': len(dup_articles) > 0  # å¯ä»¥åˆå¹¶åˆ°åŸå§‹æ–‡ä»¶
                })
        
        return {
            'analysis': usage_analysis,
            'total_duplicates': len(duplicates),
            'used_duplicates': len([a for a in usage_analysis if a['total_usage'] > 0]),
            'consolidation_candidates': len([a for a in usage_analysis if a['can_consolidate']])
        }
    
    def _find_articles_using_image(self, image_web_path: str, content_dir: Path) -> List[str]:
        """æŸ¥æ‰¾ä½¿ç”¨ç‰¹å®šå›¾ç‰‡çš„æ–‡ç« """
        articles = []
        
        for article_file in content_dir.glob("*.md"):
            try:
                with open(article_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if image_web_path in content:
                    articles.append(article_file.name)
                    
            except Exception as e:
                print(f"âš ï¸ Error reading {article_file}: {e}")
        
        return articles
    
    def generate_consolidation_plan(self, analysis: Dict) -> Dict:
        """ç”Ÿæˆé‡å¤å›¾ç‰‡æ•´åˆè®¡åˆ’"""
        plan = {
            'file_deletions': [],
            'content_replacements': [],
            'estimated_savings': 0
        }
        
        for item in analysis['analysis']:
            if not item['can_consolidate']:
                continue
            
            # è®¡åˆ’åˆ é™¤é‡å¤æ–‡ä»¶
            duplicate_path = Path(item['duplicate_file'])
            if duplicate_path.exists():
                plan['file_deletions'].append({
                    'file': str(duplicate_path),
                    'size_bytes': duplicate_path.stat().st_size,
                    'reason': f"Duplicate of {item['original_file']}"
                })
                plan['estimated_savings'] += duplicate_path.stat().st_size
            
            # è®¡åˆ’æ›¿æ¢å†…å®¹ä¸­çš„å¼•ç”¨
            for article in item['duplicate_used_in']:
                plan['content_replacements'].append({
                    'article': article,
                    'old_path': item['duplicate_web_path'],
                    'new_path': item['original_web_path'],
                    'reason': 'Consolidate to original image'
                })
        
        # è½¬æ¢å­—èŠ‚åˆ°å¯è¯»æ ¼å¼
        plan['estimated_savings_mb'] = plan['estimated_savings'] / (1024 * 1024)
        
        return plan
    
    def execute_consolidation_plan(self, plan: Dict, dry_run: bool = True, content_dir: Path = Path("content/articles")):
        """æ‰§è¡Œé‡å¤å›¾ç‰‡æ•´åˆè®¡åˆ’"""
        if dry_run:
            print("ğŸ§ª DRY RUN - No actual changes will be made")
        
        print(f"\nğŸ“‹ Consolidation Plan Summary:")
        print(f"   - Files to delete: {len(plan['file_deletions'])}")
        print(f"   - Content replacements: {len(plan['content_replacements'])}")
        print(f"   - Estimated savings: {plan['estimated_savings_mb']:.2f} MB")
        
        if not dry_run:
            # æ‰§è¡Œå†…å®¹æ›¿æ¢
            for replacement in plan['content_replacements']:
                self._replace_image_in_article(
                    article_file=content_dir / replacement['article'],
                    old_path=replacement['old_path'],
                    new_path=replacement['new_path']
                )
            
            # åˆ é™¤é‡å¤æ–‡ä»¶
            for deletion in plan['file_deletions']:
                file_path = Path(deletion['file'])
                if file_path.exists():
                    file_path.unlink()
                    print(f"ğŸ—‘ï¸ Deleted: {file_path}")
        
        return len(plan['file_deletions']) + len(plan['content_replacements'])
    
    def _replace_image_in_article(self, article_file: Path, old_path: str, new_path: str):
        """åœ¨æ–‡ç« ä¸­æ›¿æ¢å›¾ç‰‡è·¯å¾„"""
        try:
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›¿æ¢å›¾ç‰‡å¼•ç”¨
            updated_content = content.replace(old_path, new_path)
            
            if updated_content != content:
                with open(article_file, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                print(f"âœ… Updated {article_file.name}: {old_path} â†’ {new_path}")
            
        except Exception as e:
            print(f"âŒ Failed to update {article_file}: {e}")


def list_duplicates(image_root: str = 'static/images') -> List[Tuple[Path, Path]]:
    """v2 Helper function - æŸ¥æ‰¾é‡å¤å›¾ç‰‡ (å‘åå…¼å®¹)"""
    deduplicator = ContentHashDeduplicator(image_root)
    return deduplicator.find_content_duplicates()


def run_content_hash_deduplication(image_root: str = 'static/images', dry_run: bool = True, execute: bool = False):
    """è¿è¡Œå®Œæ•´çš„å†…å®¹å“ˆå¸Œå»é‡æµç¨‹"""
    print("ğŸš€ Starting Content Hash Deduplication v2")
    print("=" * 60)
    
    # 1. åˆ›å»ºå»é‡å™¨
    deduplicator = ContentHashDeduplicator(image_root)
    
    # 2. æŸ¥æ‰¾é‡å¤æ–‡ä»¶
    duplicates = deduplicator.find_content_duplicates()
    
    if not duplicates:
        print("âœ… No content duplicates found!")
        return 0
    
    # 3. åˆ†æä½¿ç”¨æƒ…å†µ
    print(f"\nğŸ“Š Analyzing usage patterns...")
    analysis = deduplicator.analyze_duplicate_usage(duplicates)
    
    print(f"   - Total duplicate pairs: {analysis['total_duplicates']}")
    print(f"   - Duplicates in use: {analysis['used_duplicates']}")
    print(f"   - Consolidation candidates: {analysis['consolidation_candidates']}")
    
    # 4. ç”Ÿæˆæ•´åˆè®¡åˆ’
    plan = deduplicator.generate_consolidation_plan(analysis)
    
    # 5. æ‰§è¡Œè®¡åˆ’ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if execute:
        changes = deduplicator.execute_consolidation_plan(plan, dry_run=dry_run)
        return changes
    else:
        # åªæ˜¾ç¤ºè®¡åˆ’
        deduplicator.execute_consolidation_plan(plan, dry_run=True)
        print(f"\nğŸ’¡ Use --execute to apply changes")
        print(f"ğŸ’¡ Use --execute --apply to apply changes permanently")
        return 0