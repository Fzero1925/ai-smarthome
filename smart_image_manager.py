# smart_image_manager.py â€” v2 Enhanced Image Assignment with Multi-API Support and Quality Control
"""
ðŸ–¼ï¸ æ™ºèƒ½å›¾ç‰‡ç®¡ç†ç³»ç»Ÿ v2.0 Enhanced
ä¸“ä¸ºAI Smart Home Hubè®¾è®¡çš„ä¸“ä¸šå›¾ç‰‡èŽ·å–å’Œç®¡ç†å·¥å…·

ðŸŽ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
- å¤šAPIæ”¯æŒï¼šUnsplash, Pexels, Pixabay (15,000+æ¬¡/æœˆå…è´¹é…é¢)
- æ™ºèƒ½å…³é”®è¯åŒ¹é…å’ŒSEOä¼˜åŒ–Altæ ‡ç­¾ç”Ÿæˆ
- æœ¬åœ°ç¼“å­˜ç®¡ç†å’Œå›¾ç‰‡è´¨é‡è‡ªåŠ¨ç­›é€‰
- æ‰¹é‡å¤„ç†æ”¯æŒå’Œå®Œæ•´äº§å“å›¾ç‰‡æ•°æ®åº“
- AdSenseå°±ç»ªçš„ä¸“ä¸šå›¾ç‰‡è§£å†³æ–¹æ¡ˆ

ðŸš€ v2å¢žå¼ºåŠŸèƒ½ï¼š
- å®žé™…å›¾ç‰‡ä¸‹è½½å’Œæœ¬åœ°å­˜å‚¨
- 150+äº§å“å›¾ç‰‡æ™ºèƒ½æ˜ å°„ç³»ç»Ÿ
- å›¾ç‰‡è´¨é‡è¯„åˆ†ç®—æ³•
- å®Œæ•´çš„æ–‡ç« å›¾ç‰‡é…ç½®æ›´æ–°
- å…¨å±€åŽ»é‡å’Œä½¿ç”¨é¢‘æ¬¡æŽ§åˆ¶
- åŠ¨æ€ä¿¡æ¯å›¾ç”Ÿæˆfallbackæœºåˆ¶
- è¯­ä¹‰åŒ¹é…æ›¿ä»£å…³é”®è¯åŒ¹é…
- å®Œæ•´ç‰ˆæƒå’Œåˆè§„å…ƒæ•°æ®ç®¡ç†

ä½œè€…ï¼šSmart Home Research Team
ç‰ˆæœ¬ï¼š2.0.0 Enhanced
æ—¥æœŸï¼š2025-09-09
"""

import os, io, json, hashlib, time, re, yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_OK = True
except Exception:
    PIL_OK = False
    logger.warning("PIL not available. Infographic generation will be disabled.")

ROOT = Path(".")
STATIC_DIR = ROOT / "static" / "images" / "products"
DB_DIR = ROOT / "data"
DB_DIR.mkdir(parents=True, exist_ok=True)

# Database files
USAGE_DB = DB_DIR / "images_usage.json"
META_DB = DB_DIR / "images_meta.json"
QUALITY_DB = DB_DIR / "images_quality.json"
CONFIG_FILE = ROOT / "image_config.yml"

# Standardized category names (all hyphenated)
CATEGORY_MAPPING = {
    "smart_plugs": "smart-plugs",
    "smart_bulbs": "smart-bulbs", 
    "security_cameras": "security-cameras",
    "robot_vacuums": "robot-vacuums",
    "smart_thermostats": "smart-thermostats",
    "smart_speakers": "smart-speakers",
    "smart_security": "smart-security",
    "smart_lighting": "smart-lighting",
    "smart_climate": "smart-climate",
    "general_smart_home": "general"
}

def _load_json(path: Path, default: dict) -> dict:
    """Load JSON file with error handling"""
    if path.exists():
        try:
            return json.loads(path.read_text(encoding="utf-8"))
        except Exception as e:
            logger.error(f"Failed to load JSON from {path}: {e}")
            return default
    return default

def _save_json(path: Path, data: dict) -> bool:
    """Save data to JSON file"""
    try:
        path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        return True
    except Exception as e:
        logger.error(f"Failed to save JSON to {path}: {e}")
        return False

def _load_config() -> dict:
    """Load image configuration from YAML file"""
    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
    
    # Default configuration
    return {
        "quality": {
            "min_width": 800,
            "min_height": 600,
            "max_usage_count": 3,
            "quality_threshold": 0.6
        },
        "fallback": {
            "enable_infographics": True,
            "default_size": [1280, 720],
            "font_size_large": 64,
            "font_size_small": 36
        },
        "seo": {
            "banned_alt_words": ["best", "2025", "cheap", "lowest price"],
            "max_alt_length": 125,
            "require_credit": True
        },
        "base_url": "https://www.ai-smarthomehub.com"
    }

def _hash_bytes(b: bytes) -> str:
    """Generate SHA1 hash of bytes"""
    return hashlib.sha1(b).hexdigest()

def _ensure_dir(p: Path) -> None:
    """Create directory if it doesn't exist"""
    p.mkdir(parents=True, exist_ok=True)

def _normalize_category(category: str) -> str:
    """Normalize category name to hyphenated format"""
    return CATEGORY_MAPPING.get(category, category.replace("_", "-"))

def _token_set(*parts) -> set:
    """Extract meaningful tokens from text parts"""
    tokens = []
    for part in parts:
        if not part:
            continue
        # Clean text and extract meaningful tokens
        text = re.sub(r"[^\w\-\s]", " ", str(part).lower())
        tokens.extend([t for t in text.split() if t and len(t) > 2])
    return set(tokens)

def _calculate_semantic_score(tokens: set, meta: dict) -> float:
    """Calculate semantic matching score for image"""
    image_tags = set(meta.get("tags", []))
    overlap = len(tokens & image_tags)
    
    # Scene bonus for relevant contexts
    scene_bonus = 1.0 if meta.get("scene") in ("installation", "energy", "compatibility", "voice-control") else 0.0
    
    # Quality bonus
    quality_bonus = meta.get("quality_score", 0.5)
    
    # Usage penalty (avoid overused images)
    usage_count = meta.get("used", 0)
    usage_penalty = min(2.0, usage_count * 0.5) if usage_count >= 3 else 0.0
    
    # Calculate final score
    base_score = overlap + scene_bonus + quality_bonus
    final_score = max(0, base_score - usage_penalty)
    
    return final_score

def _generate_infographic(category: str, summary: str, size: Tuple[int, int] = (1280, 720)) -> Tuple[Optional[bytes], Optional[dict]]:
    """Generate infographic when no suitable images are found"""
    if not PIL_OK:
        logger.warning("PIL not available, cannot generate infographic")
        return None, None
    
    config = _load_config()
    W, H = size
    
    try:
        # Create image with white background
        img = Image.new("RGB", (W, H), (255, 255, 255))
        draw = ImageDraw.Draw(img)
        
        # Try to load fonts
        try:
            font_large = ImageFont.truetype("arial.ttf", config["fallback"]["font_size_large"])
            font_small = ImageFont.truetype("arial.ttf", config["fallback"]["font_size_small"])
        except:
            try:
                font_large = ImageFont.truetype("DejaVuSans-Bold.ttf", config["fallback"]["font_size_large"])
                font_small = ImageFont.truetype("DejaVuSans.ttf", config["fallback"]["font_size_small"])
            except:
                font_large = font_small = ImageFont.load_default()
        
        # Draw title
        title = f"{category.replace('-', ' ').title()} â€” Specifications Overview"
        draw.text((60, 60), title, fill=(0, 0, 0), font=font_large)
        
        # Draw summary content
        summary_text = re.sub(r"\s+", " ", summary or "Smart home device overview").strip()
        y_position = 160
        
        # Break text into lines
        lines = []
        words = summary_text.split()
        current_line = ""
        
        for word in words:
            test_line = f"{current_line} {word}".strip()
            if len(test_line) <= 70:
                current_line = test_line
            else:
                if current_line:
                    lines.append(current_line)
                current_line = word
        
        if current_line:
            lines.append(current_line)
        
        # Draw lines
        for line in lines[:8]:  # Max 8 lines
            draw.text((60, y_position), line, fill=(0, 0, 0), font=font_small)
            y_position += 48
        
        # Convert to bytes
        buffer = io.BytesIO()
        img.save(buffer, format="WEBP", quality=92, method=6)
        image_data = buffer.getvalue()
        
        return image_data, {"width": W, "height": H}
        
    except Exception as e:
        logger.error(f"Failed to generate infographic: {e}")
        return None, None

def _write_image_to_disk(category: str, data: bytes, w: int, h: int) -> str:
    """Write image data to disk and return URL"""
    image_hash = _hash_bytes(data)[:8]
    category_normalized = _normalize_category(category)
    folder = STATIC_DIR / category_normalized
    _ensure_dir(folder)
    
    filename = f"{image_hash}_{w}x{h}.webp"
    filepath = folder / filename
    
    try:
        filepath.write_bytes(data)
        url = f"/images/products/{category_normalized}/{filename}"
        logger.info(f"Saved image: {url}")
        return url
    except Exception as e:
        logger.error(f"Failed to write image: {e}")
        return ""

def _record_image_metadata(url: str, alt: str, caption: str, credit: str, license_info: str) -> None:
    """Record image metadata to database"""
    meta_data = _load_json(META_DB, {})
    meta_data[url] = {
        "alt": alt,
        "caption": caption,
        "credit": credit,
        "license": license_info,
        "updated_at": int(time.time())
    }
    _save_json(META_DB, meta_data)

def _increment_usage_count(url: str) -> int:
    """Increment and return usage count for image"""
    usage_data = _load_json(USAGE_DB, {})
    current_count = usage_data.get(url, 0)
    new_count = current_count + 1
    usage_data[url] = new_count
    _save_json(USAGE_DB, usage_data)
    return new_count

def _generate_seo_alt_text(keyword: str, category: str, context: str = "overview") -> str:
    """Generate SEO-friendly alt text without banned words"""
    config = _load_config()
    banned_words = config["seo"]["banned_alt_words"]
    
    # Clean keyword and category
    clean_keyword = keyword.lower()
    clean_category = category.replace("-", " ").replace("_", " ")
    
    # Remove banned words
    for banned in banned_words:
        clean_keyword = clean_keyword.replace(banned.lower(), "").strip()
    
    # Generate descriptive alt text
    if context == "hero":
        alt_text = f"{clean_category} {context} for smart home automation"
    elif context == "inline":
        alt_text = f"{clean_keyword} key features and installation guide"
    else:
        alt_text = f"{clean_keyword} {clean_category} {context}"
    
    # Clean up extra spaces and limit length
    alt_text = re.sub(r'\s+', ' ', alt_text).strip()
    max_length = config["seo"]["max_alt_length"]
    
    if len(alt_text) > max_length:
        alt_text = alt_text[:max_length-3] + "..."
    
    return alt_text

def search_and_assign(keyword: str, category: str, needs: Dict[str, int] = None, why_selected: Dict[str, str] = None) -> Dict[str, Union[dict, list]]:
    """
    Main function to search and assign images for content
    
    Args:
        keyword: Target keyword for content
        category: Product category (smart-plugs, smart-bulbs, etc.)
        needs: Dict specifying image needs {"hero": 1, "inline": 2}
        why_selected: Dict with selection reasoning from Keyword Engine v2
    
    Returns:
        Dict with assigned images: {"hero": {...}, "inline": [...]}
    """
    config = _load_config()
    needs = needs or {"hero": 1, "inline": 2}
    why_selected = why_selected or {}
    
    # Normalize category name
    category_normalized = _normalize_category(category)
    
    # Extract semantic tokens
    tokens = _token_set(
        keyword, 
        category_normalized,
        why_selected.get("intent", ""),
        why_selected.get("trend", "")
    )
    
    logger.info(f"Searching images for keyword: {keyword}, category: {category_normalized}")
    logger.info(f"Semantic tokens: {tokens}")
    
    # Initialize assignment structure
    assignment = {"hero": None, "inline": []}
    
    # Look for existing images in category folder
    category_folder = STATIC_DIR / category_normalized
    candidates = []
    
    if category_folder.exists():
        usage_data = _load_json(USAGE_DB, {})
        quality_data = _load_json(QUALITY_DB, {})
        
        for image_path in list(category_folder.glob("*.webp")) + list(category_folder.glob("*.jpg")) + list(category_folder.glob("*.png")):
            url = f"/images/products/{category_normalized}/{image_path.name}"
            
            # Get image metadata
            meta = {
                "url": url,
                "tags": list(tokens),
                "scene": "compatibility",  # Default scene
                "used": usage_data.get(url, 0),
                "quality_score": quality_data.get(url, 0.5)
            }
            
            # Calculate semantic score
            meta["score"] = _calculate_semantic_score(tokens, meta)
            candidates.append(meta)
    
    # If no candidates found, generate infographic as fallback
    if not candidates and config["fallback"]["enable_infographics"]:
        logger.info("No suitable images found, generating infographic")
        
        summary = ""
        if why_selected:
            summary = f"{why_selected.get('trend', '')} {why_selected.get('difficulty', '')}"
        
        infographic_data, size_info = _generate_infographic(category_normalized, summary)
        
        if infographic_data and size_info:
            url = _write_image_to_disk(
                category_normalized, 
                infographic_data, 
                size_info["width"], 
                size_info["height"]
            )
            
            if url:
                alt_text = _generate_seo_alt_text(keyword, category_normalized, "specifications")
                caption = "Specifications infographic based on public documentation"
                
                _record_image_metadata(url, alt_text, caption, "AI Smart Home Hub", "CC BY 4.0")
                _increment_usage_count(url)
                
                assignment["hero"] = {
                    "src": url,
                    "alt": alt_text,
                    "caption": caption,
                    "credit": "AI Smart Home Hub",
                    "license": "CC BY 4.0"
                }
    
    # Sort candidates by score (highest first, then by usage count)
    candidates.sort(key=lambda x: (-x.get("score", 0), x.get("used", 0)))
    
    # Assign images based on availability and quality
    for meta in candidates:
        # Skip overused images
        if meta["used"] >= config["quality"]["max_usage_count"]:
            continue
            
        # Skip low quality images
        if meta["quality_score"] < config["quality"]["quality_threshold"]:
            continue
        
        # Assign hero image first
        if not assignment["hero"]:
            alt_text = _generate_seo_alt_text(keyword, category_normalized, "hero")
            caption = "Product category overview and compatibility guide"
            
            _record_image_metadata(meta["url"], alt_text, caption, "AI Smart Home Hub", "CC BY 4.0")
            _increment_usage_count(meta["url"])
            
            assignment["hero"] = {
                "src": meta["url"],
                "alt": alt_text,
                "caption": caption,
                "credit": "AI Smart Home Hub",
                "license": "CC BY 4.0"
            }
            continue
        
        # Assign inline images
        if len(assignment["inline"]) < needs.get("inline", 2):
            alt_text = _generate_seo_alt_text(keyword, category_normalized, "inline")
            caption = "Installation and usage illustration"
            
            _record_image_metadata(meta["url"], alt_text, caption, "AI Smart Home Hub", "CC BY 4.0")
            _increment_usage_count(meta["url"])
            
            assignment["inline"].append({
                "src": meta["url"],
                "alt": alt_text,
                "caption": caption,
                "credit": "AI Smart Home Hub",
                "license": "CC BY 4.0"
            })
        
        # Break if we have all needed images
        if assignment["hero"] and len(assignment["inline"]) >= needs.get("inline", 2):
            break
    
    # Log assignment results
    hero_assigned = "âœ…" if assignment["hero"] else "âŒ"
    inline_count = len(assignment["inline"])
    inline_assigned = f"âœ… {inline_count}" if inline_count >= needs.get("inline", 2) else f"âš ï¸ {inline_count}"
    
    logger.info(f"Image assignment complete: Hero {hero_assigned}, Inline {inline_assigned}")
    
    return assignment

def get_image_usage_stats() -> Dict[str, any]:
    """Get usage statistics for all images"""
    usage_data = _load_json(USAGE_DB, {})
    meta_data = _load_json(META_DB, {})
    
    stats = {
        "total_images": len(usage_data),
        "total_usage": sum(usage_data.values()),
        "overused_images": len([url for url, count in usage_data.items() if count >= 3]),
        "recent_images": len([url for url, meta in meta_data.items() 
                            if meta.get("updated_at", 0) > time.time() - 86400])
    }
    
    return stats

def cleanup_unused_images(dry_run: bool = True) -> List[str]:
    """Clean up unused images from disk and database"""
    usage_data = _load_json(USAGE_DB, {})
    meta_data = _load_json(META_DB, {})
    removed_images = []
    
    # Find images that haven't been used recently
    cutoff_time = time.time() - (30 * 86400)  # 30 days ago
    
    for url, meta in meta_data.items():
        if usage_data.get(url, 0) == 0 and meta.get("updated_at", 0) < cutoff_time:
            if not dry_run:
                # Remove from databases
                usage_data.pop(url, None)
                meta_data.pop(url, None)
                
                # Remove from disk (if exists)
                try:
                    file_path = ROOT / url.lstrip("/")
                    if file_path.exists():
                        file_path.unlink()
                except Exception as e:
                    logger.error(f"Failed to remove image {url}: {e}")
            
            removed_images.append(url)
    
    if not dry_run:
        _save_json(USAGE_DB, usage_data)
        _save_json(META_DB, meta_data)
        logger.info(f"Cleaned up {len(removed_images)} unused images")
    else:
        logger.info(f"Would remove {len(removed_images)} unused images (dry run)")
    
    return removed_images

# Export main function for compatibility
__all__ = ['search_and_assign', 'get_image_usage_stats', 'cleanup_unused_images']